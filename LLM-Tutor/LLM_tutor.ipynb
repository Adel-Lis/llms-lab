{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# LLM Personalized Tutor in Coding and Artificial Intelligence\n",
    "\n",
    "In this small project, I demonstrate the ability to use the OpenAI API and Ollama in order to build a tool that takes a technical question, and responds by following a layout and other characteristics.\n",
    "\n",
    "For this project I customized two famous LLM models: \n",
    "- ***gpt-4o-mini***\n",
    "- ***llama 3.2***\n",
    "\n",
    "I have also enables stream output *only for gpt-4o-mini* in order to study the difference in response and User Interraction. <u>The answer will always be returned as Markdown and then rendered by the IPython.display libraries in this notebook.</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f5e3ce",
   "metadata": {},
   "source": [
    "> This tool is gonna be able to answer questions on code and llms, and it will act as a customized co-pilot. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78264e28",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660bd2c6",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eacaa83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key loaded\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if api_key and api_key.startswith('sk-proj-') and len(api_key)>10:\n",
    "    print(\"API key loaded\")\n",
    "else:\n",
    "    print(\"There might be a problem with your API key. It was not found\")\n",
    "\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7046d2",
   "metadata": {},
   "source": [
    "### Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bb0211",
   "metadata": {},
   "source": [
    "This promps will set how the LLM should behave and responde and what to expect the question to be about. For this particular case, the LLM is customize with ***zero-shot prompting***, in fact, I only specified how I want the answer to be structured, but I do not provide additional examples to support that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are provided with a coding and/or LLM problem as a string input. You are an expert of Computer Science, Artificial Intelligence and LLM Engineering fields. \\\n",
    "You are able to break down the problem and make it easier for the user \\\n",
    "You should be able to answer with a simple, straight to the point answer and solution to the problem, in addition, you should return *multiple examples* that shows different use cases of the answer and are meaningful to explain better the problem. \\\n",
    "Then, you go more in depth by explaining in-depth theory specific to the topic that you are treating.\\n \\\n",
    "You are able to explain everything like a professor that would make the extra effort for the user to understand. Use a friendly and simple vocabulary.\\n\"\n",
    "system_prompt += \"Respond in a well formatted markdown and use separate lines between the quick explanation-solution part and more in-depth part. Any code example should be added to the Markdown 'fenced code blocks' with the correct coding language identified (if none specified you use Python for your examples)\"\n",
    "\n",
    "user_prompt = \"You are given a technical question that can represent a problem, issue, or request in the Artificial Intelligence or Coding field. You help the user by carefully answering the following question:\"\n",
    "\n",
    "def get_truncated_user_prompt(prompt):\n",
    "    return prompt if len(prompt) <= 5000 else prompt[:5000] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d1ce50",
   "metadata": {},
   "source": [
    "### GPT-4o-mini Function Call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa41f8c",
   "metadata": {},
   "source": [
    "The following function makes a call to gpt-4o-mini api with the required system and user configuration prompts. *Stream* is enabled, so the answer returned in output is immediately displayed (token by token). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_4o_problem_answer(question):\n",
    "    stream = openai.chat.completions.create(\n",
    "        model=MODEL_GPT,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": get_truncated_user_prompt(f\"{user_prompt} {question}\")}\n",
    "        ],\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        # response = response.replace(\"```markdown\", \"\")\n",
    "        update_display(Markdown(response), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d3cd72",
   "metadata": {},
   "source": [
    "### Ollama Function Call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca856ff2",
   "metadata": {},
   "source": [
    "The following method calls the Ollama local API with the configured system and user prompts. This function *does not activate stream* the answer, therefore everything will be rendered to Markdown and displayed only after all the answer was retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ollama_problem_answer(question):\n",
    "    response = ollama.chat(\n",
    "        model=MODEL_LLAMA,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": get_truncated_user_prompt(f\"{user_prompt} {question}\")}\n",
    "        ],\n",
    "        stream=False\n",
    "    )\n",
    "\n",
    "    display(Markdown(response[\"message\"][\"content\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb711a6",
   "metadata": {},
   "source": [
    "### Make a dynamic call that uses the model the user wants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d04a4a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(problem, model = \"ollama\"):\n",
    "    if model == \"gpt-4o\":\n",
    "        gpt_4o_problem_answer(question=problem)\n",
    "    elif model == \"ollama\":\n",
    "        ollama_problem_answer(question=problem)\n",
    "    else:\n",
    "        print(\"The model you want to interrogate has not been implemented yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07e9274",
   "metadata": {},
   "source": [
    "### User input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Quick Explanation-Solution**\n",
       "==========================\n",
       "\n",
       "The provided code uses a technique called **generator expression** to extract author names from a list of books. Here's a breakdown:\n",
       "\n",
       "* `yield from` is used to delegate the iteration to another iterable (in this case, the generator expression).\n",
       "* `{book.get(\"author\") for book in books if book.get(\"author\")}` is a generator expression that:\n",
       "\t+ Iterates over each book in the `books` list.\n",
       "\t+ Filters out books with missing author information using the `if` condition.\n",
       "\t+ Extracts the author name from each book using the `get()` method.\n",
       "\n",
       "The resulting output will be an iterator that yields the author names of the books with available information.\n",
       "\n",
       "**Example Use Cases**\n",
       "--------------------\n",
       "\n",
       "```python\n",
       "# Sample data\n",
       "books = [\n",
       "    {\"title\": \"Book 1\", \"author\": \"John Doe\"},\n",
       "    {\"title\": \"Book 2\", \"author\": None},\n",
       "    {\"title\": \"Book 3\", \"author\": \"Jane Smith\"}\n",
       "]\n",
       "\n",
       "# Using the generator expression to extract author names\n",
       "authors = yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
       "print(authors)  # Output: ['John Doe', 'Jane Smith']\n",
       "```\n",
       "\n",
       "```python\n",
       "# Using the generator expression in a loop\n",
       "for author in yield from {book.get(\"author\") for book in books if book.get(\"author\")}:\n",
       "    print(author)\n",
       "# Output:\n",
       "# John Doe\n",
       "# Jane Smith\n",
       "```\n",
       "\n",
       "**More In-Depth Theory**\n",
       "----------------------\n",
       "\n",
       "Generator expressions are a powerful tool in Python that allow you to write concise and efficient code. They consist of a subexpression enclosed in parentheses, which is executed only when the resulting iterator is requested.\n",
       "\n",
       "The `yield from` keyword is used to delegate the iteration to another iterable, allowing you to nest generator expressions or combine them with loops.\n",
       "\n",
       "In this specific example, we use a generator expression to filter out books with missing author information. The `if book.get(\"author\")` condition ensures that only books with available author data are processed.\n",
       "\n",
       "By using `yield from`, we can simplify the code and avoid creating unnecessary intermediate lists or data structures, making it more memory-efficient and scalable for large datasets.\n",
       "\n",
       "In general, generator expressions are useful when:\n",
       "\n",
       "* You need to process large datasets and want to avoid loading them into memory.\n",
       "* You want to perform complex filtering or transformations on data without storing temporary results.\n",
       "* You need to write concise and readable code that is easy to maintain and extend."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Write your question here\n",
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\"\n",
    "\n",
    "# Choose your model. Available:\n",
    "# - GPT-4o : \"gpt-4o\"\n",
    "# - ollama : \"ollama\"\n",
    "use_model = \"ollama\"\n",
    "\n",
    "generate_answer(question, use_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4d58ff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Quick Explanation\n",
       "\n",
       "Transformers are a type of deep learning model introduced in 2017 that revolutionized how we approach natural language processing (NLP) tasks. Unlike previous models that processed sequences in order, Transformers use a mechanism called self-attention to weigh the importance of different words in relation to each other, allowing them to capture long-range dependencies in the data more efficiently.\n",
       "\n",
       "Transformers form the backbone of many state-of-the-art architectures, such as BERT and GPT, which are widely used in applications like translation, summarization, and conversation systems.\n",
       "\n",
       "#### How Transformers Work:\n",
       "1. **Self-Attention**: This mechanism evaluates the relationships between all words in a sequence simultaneously, allowing the model to focus on important words more efficiently.\n",
       "2. **Positional Encoding**: Since Transformers do not inherently understand the sequence of words, positional encodings are added to the input embeddings to give the model information about the position of each word.\n",
       "3. **Multi-Head Attention**: The model applies multiple self-attention mechanisms in parallel to capture different aspects of word relationships.\n",
       "4. **Feed-Forward Networks**: After attention, the output is passed through feed-forward neural networks, which helps in processing the information further.\n",
       "5. **Layer Normalization and Residual Connections**: These techniques help stabilize and accelerate training.\n",
       "\n",
       "### Example Use Cases\n",
       "1. **Text Generation**: Generating coherent and contextually relevant text using models like GPT-3.\n",
       "   ```python\n",
       "   from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
       "\n",
       "   model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
       "   tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
       "\n",
       "   input_ids = tokenizer.encode(\"Once upon a time\", return_tensors='pt')\n",
       "   outputs = model.generate(input_ids, max_length=50)\n",
       "   generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
       "   print(generated_text)\n",
       "   ```\n",
       "\n",
       "2. **Sentiment Analysis**: Analyzing the sentiment of a given text using fine-tuned BERT.\n",
       "   ```python\n",
       "   from transformers import BertForSequenceClassification, BertTokenizer\n",
       "   from torch.nn.functional import softmax\n",
       "   import torch\n",
       "\n",
       "   model = BertForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
       "   tokenizer = BertTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
       "\n",
       "   text = \"I love this product!\"\n",
       "   inputs = tokenizer(text, return_tensors='pt')\n",
       "   outputs = model(**inputs)\n",
       "   probabilities = softmax(outputs.logits, dim=1)\n",
       "   print(probabilities)\n",
       "   ```\n",
       "\n",
       "3. **Machine Translation**: Translating sentences between languages, effectively utilizing the Transformer architecture for this task.\n",
       "   ```python\n",
       "   from transformers import MarianMTModel, MarianTokenizer\n",
       "\n",
       "   model_name = 'Helsinki-NLP/opus-mt-en-fr'\n",
       "   model = MarianMTModel.from_pretrained(model_name)\n",
       "   tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
       "\n",
       "   text = \"Hello, how are you?\"\n",
       "   translated = model.generate(**tokenizer.encode(text, return_tensors='pt'))\n",
       "   print(tokenizer.decode(translated[0], skip_special_tokens=True))  # \"Bonjour, comment Ã§a va ?\"\n",
       "   ```\n",
       "\n",
       "---\n",
       "\n",
       "### In-Depth Explanation\n",
       "\n",
       "#### Key Concepts in Transformers\n",
       "\n",
       "1. **Self-Attention**:\n",
       "   - This mechanism allows the model to weigh the importance of each word relative to others in the sequence. \n",
       "   - It computes a score for every pair of words and generates a context vector for each word based on these scores, which helps capture meanings that can depend on words far away in the sequence.\n",
       "\n",
       "2. **Positional Encoding**:\n",
       "   - Since Transformers process input in parallel (unlike RNNs that go step-by-step), they require a way to keep track of the order of words. \n",
       "   - Positional embeddings are added to the input embeddings to inject information about the position of each token.\n",
       "\n",
       "3. **Multi-Head Attention**:\n",
       "   - This allows the model to attend to different parts of the input word embeddings simultaneously. \n",
       "   - Each \"head\" can potentially learn to focus on different aspects like syntax or semantics, enriching the representation.\n",
       "\n",
       "4. **Feed-Forward Networks**:\n",
       "   - Each attention layer is followed by a feed-forward neural network that processes the output to further transform it before passing it to the next layer.\n",
       "\n",
       "5. **Layer Normalization and Residual Connections**:\n",
       "   - Residual connections help stabilize training by allowing gradients to flow through the network more easily, while layer normalization normalizes the outputs to maintain consistent distributions.\n",
       "\n",
       "6. **Encoder-Decoder Structure**:\n",
       "   - The Transformer can have an encoder-decoder architecture, where the encoder processes the input sequence and the decoder generates the output sequence. This is particularly useful in tasks like machine translation.\n",
       "\n",
       "#### Why Transformers are Important in Deep Learning\n",
       "\n",
       "The introduction of Transformers has led to significant improvements in handling sequences due to their ability to capture context and relationships without the limitations imposed by recurrent structures. They excel at:\n",
       "- **Scalability**: Full parallel processing during training reduces training time significantly for large datasets.\n",
       "- **Performance**: Achieving state-of-the-art results in numerous NLP tasks.\n",
       "- **Transfer Learning**: Models like BERT and GPT leverage pre-trained weights, allowing them to be fine-tuned on specialized tasks with relatively small datasets.\n",
       "\n",
       "Overall, Transformers represent a crucial advancement in deep learning architecture, enabling breakthroughs in a variety of applications from text understanding to image processing."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Write your question here\n",
    "question = \"\"\"\n",
    "Can you explain me the concept of Transformers in Deep Learning, how they work and why Deep Learning relies on this in order to work ?\n",
    "\"\"\"\n",
    "\n",
    "# Choose your model. Available:\n",
    "# - GPT-4o : \"gpt-4o\"\n",
    "# - ollama : \"ollama\"\n",
    "use_model = \"gpt-4o\"\n",
    "\n",
    "generate_answer(question, use_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
