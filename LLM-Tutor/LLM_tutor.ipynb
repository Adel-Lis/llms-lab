{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# LLM Personalized Tutor in Coding and Artificial Intelligence\n",
    "\n",
    "In this small project, I demonstrate the ability to use the OpenAI API and Ollama in order to build a tool that takes a technical question, and responds by following a layout and other characteristics.\n",
    "\n",
    "For this project I customized two famous LLM models: \n",
    "- ***gpt-4o-mini***\n",
    "- ***llama 3.2***\n",
    "\n",
    "I have also enables stream output *only for gpt-4o-mini* in order to study the difference in response and User Interraction. <u>The answer will always be returned as Markdown and then rendered by the IPython.display libraries in this notebook.</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f5e3ce",
   "metadata": {},
   "source": [
    "> This tool is gonna be able to answer questions on code and llms, and it will act as a customized co-pilot. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78264e28",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI\n",
    "import ollama\n",
    "import anthropic\n",
    "from google import genai\n",
    "from google.genai import types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660bd2c6",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'\n",
    "MODEL_ANTHROPIC = 'claude-3-5-haiku-latest' # https://docs.claude.com/en/docs/about-claude/models/overview\n",
    "MODEL_GOOGLE = 'gemini-2.5-flash' # https://ai.google.dev/gemini-api/docs/models?authuser=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eacaa83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API key loaded\n",
      "Anthropic API key loaded\n",
      "Gemini API key loaded\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "google_api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "if openai_api_key and openai_api_key.startswith('sk-proj-') and len(openai_api_key)>10:\n",
    "    print(\"OpenAI API key loaded\")\n",
    "else:\n",
    "    print(\"There might be a problem with your OpenAI API key. It was not found\")\n",
    "\n",
    "if anthropic_api_key and anthropic_api_key.startswith('sk-ant-') and len(anthropic_api_key)>10:\n",
    "    print(\"Anthropic API key loaded\")\n",
    "else:\n",
    "    print(\"There might be a problem with your Anthropic API key. It was not found\")\n",
    "\n",
    "if google_api_key and len(google_api_key)>10:\n",
    "    print(\"Gemini API key loaded\")\n",
    "else:\n",
    "    print(\"There might be a problem with your Gemini API key. It was not found\")\n",
    "\n",
    "openai = OpenAI()\n",
    "anthr_claude = client = anthropic.Anthropic(api_key=anthropic_api_key)\n",
    "gemini = genai.Client(api_key=google_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7046d2",
   "metadata": {},
   "source": [
    "### Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bb0211",
   "metadata": {},
   "source": [
    "This promps will set how the LLM should behave and responde and what to expect the question to be about. For this particular case, the LLM is customize with ***zero-shot prompting***, in fact, I only specified how I want the answer to be structured, but I do not provide additional examples to support that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are provided with a coding and/or LLM problem as a string input. You are an expert of Computer Science, Artificial Intelligence and LLM Engineering fields. \\\n",
    "You are able to break down the problem and make it easier for the user \\\n",
    "You should be able to answer with a simple, straight to the point answer and solution to the problem, in addition, you should return *multiple examples* that shows different use cases of the answer and are meaningful to explain better the problem. \\\n",
    "Then, you go more in depth by explaining in-depth theory specific to the topic that you are treating.\\n \\\n",
    "You are able to explain everything like a professor that would make the extra effort for the user to understand. Use a friendly and simple vocabulary.\\n\"\n",
    "system_prompt += \"Respond in a well formatted markdown and use separate lines between the quick explanation-solution part and more in-depth part. Any code example should be added to the Markdown 'fenced code blocks' with the correct coding language identified (if none specified you use Python for your examples)\"\n",
    "\n",
    "user_prompt = \"You are given a technical question that can represent a problem, issue, or request in the Artificial Intelligence or Coding field. You help the user by carefully answering the following question:\"\n",
    "\n",
    "def get_truncated_user_prompt(prompt):\n",
    "    return prompt if len(prompt) <= 5000 else prompt[:5000] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d1ce50",
   "metadata": {},
   "source": [
    "### GPT-4o-mini Function Call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa41f8c",
   "metadata": {},
   "source": [
    "The following function makes a call to gpt-4o-mini api with the required system and user configuration prompts. *Stream* is enabled, so the answer returned in output is immediately displayed (token by token). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_4o_problem_answer(question):\n",
    "    stream = openai.chat.completions.create(\n",
    "        model=MODEL_GPT,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": get_truncated_user_prompt(f\"{user_prompt} {question}\")}\n",
    "        ],\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        # response = response.replace(\"```markdown\", \"\")\n",
    "        update_display(Markdown(response), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d3cd72",
   "metadata": {},
   "source": [
    "### Ollama Function Call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca856ff2",
   "metadata": {},
   "source": [
    "The following method calls the Ollama local API with the configured system and user prompts. This function *does not activate stream* the answer, therefore everything will be rendered to Markdown and displayed only after all the answer was retrieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ollama_problem_answer(question):\n",
    "    response = ollama.chat(\n",
    "        model=MODEL_LLAMA,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": get_truncated_user_prompt(f\"{user_prompt} {question}\")}\n",
    "        ],\n",
    "        stream=False\n",
    "    )\n",
    "\n",
    "    display(Markdown(response[\"message\"][\"content\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3dc030",
   "metadata": {},
   "source": [
    "### Anthropic Function Call\n",
    "\n",
    "to write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ff03a586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anthropic_problem_answer(question):\n",
    "    message = anthr_claude.messages.create(\n",
    "        model=MODEL_ANTHROPIC,\n",
    "        max_tokens=1024,\n",
    "        system=system_prompt,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": get_truncated_user_prompt(f\"{user_prompt} {question}\")}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    display(Markdown(message.content[0].text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c5c9f0",
   "metadata": {},
   "source": [
    "### Gemini Function Call\n",
    "\n",
    "to write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3e121f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gemini_problem_answer(question):\n",
    "    user_prompt_gemini = get_truncated_user_prompt(f\"{user_prompt} {question}\")\n",
    "\n",
    "    response = gemini.models.generate_content(\n",
    "        model=MODEL_GOOGLE,\n",
    "        contents=user_prompt_gemini,\n",
    "        config=types.GenerateContentConfig(system_instruction=system_prompt, temperature=0.7)\n",
    "    )\n",
    "\n",
    "    display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb711a6",
   "metadata": {},
   "source": [
    "### Make a dynamic call that uses the model the user wants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d04a4a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(problem, model = \"ollama\"):\n",
    "    if model == \"gpt-4o\":\n",
    "        gpt_4o_problem_answer(question=problem)\n",
    "    elif model == \"ollama\":\n",
    "        ollama_problem_answer(question=problem)\n",
    "    elif model == \"claude-haiku\":\n",
    "        anthropic_problem_answer(question=problem)\n",
    "    elif model == \"gemini-flash-2.5\":\n",
    "        gemini_problem_answer(question=problem)\n",
    "    else:\n",
    "        print(\"The model you want to interrogate has not been implemented yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07e9274",
   "metadata": {},
   "source": [
    "## User input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Quick Explanation-Solution**\n",
       "==========================\n",
       "\n",
       "The provided code uses a technique called **generator expression** to extract author names from a list of books. Here's a breakdown:\n",
       "\n",
       "* `yield from` is used to delegate the iteration to another iterable (in this case, the generator expression).\n",
       "* `{book.get(\"author\") for book in books if book.get(\"author\")}` is a generator expression that:\n",
       "\t+ Iterates over each book in the `books` list.\n",
       "\t+ Filters out books with missing author information using the `if` condition.\n",
       "\t+ Extracts the author name from each book using the `get()` method.\n",
       "\n",
       "The resulting output will be an iterator that yields the author names of the books with available information.\n",
       "\n",
       "**Example Use Cases**\n",
       "--------------------\n",
       "\n",
       "```python\n",
       "# Sample data\n",
       "books = [\n",
       "    {\"title\": \"Book 1\", \"author\": \"John Doe\"},\n",
       "    {\"title\": \"Book 2\", \"author\": None},\n",
       "    {\"title\": \"Book 3\", \"author\": \"Jane Smith\"}\n",
       "]\n",
       "\n",
       "# Using the generator expression to extract author names\n",
       "authors = yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
       "print(authors)  # Output: ['John Doe', 'Jane Smith']\n",
       "```\n",
       "\n",
       "```python\n",
       "# Using the generator expression in a loop\n",
       "for author in yield from {book.get(\"author\") for book in books if book.get(\"author\")}:\n",
       "    print(author)\n",
       "# Output:\n",
       "# John Doe\n",
       "# Jane Smith\n",
       "```\n",
       "\n",
       "**More In-Depth Theory**\n",
       "----------------------\n",
       "\n",
       "Generator expressions are a powerful tool in Python that allow you to write concise and efficient code. They consist of a subexpression enclosed in parentheses, which is executed only when the resulting iterator is requested.\n",
       "\n",
       "The `yield from` keyword is used to delegate the iteration to another iterable, allowing you to nest generator expressions or combine them with loops.\n",
       "\n",
       "In this specific example, we use a generator expression to filter out books with missing author information. The `if book.get(\"author\")` condition ensures that only books with available author data are processed.\n",
       "\n",
       "By using `yield from`, we can simplify the code and avoid creating unnecessary intermediate lists or data structures, making it more memory-efficient and scalable for large datasets.\n",
       "\n",
       "In general, generator expressions are useful when:\n",
       "\n",
       "* You need to process large datasets and want to avoid loading them into memory.\n",
       "* You want to perform complex filtering or transformations on data without storing temporary results.\n",
       "* You need to write concise and readable code that is easy to maintain and extend."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Write your question here\n",
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\"\n",
    "\n",
    "# Choose your model. Available:\n",
    "# - GPT-4o : \"gpt-4o\"\n",
    "# - ollama : \"ollama\"\n",
    "# - claude-haiku-3.5 : \"claude-haiku\"\n",
    "# - gemini-flash-2.5 : \"gemini-2.5\"\n",
    "use_model = \"ollama\"\n",
    "\n",
    "generate_answer(question, use_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e4d58ff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Quick Explanation and Solution\n",
       "\n",
       "Transformers are a type of model architecture used primarily in Natural Language Processing (NLP) that allows machines to understand and generate human languages. They are recognized for their ability to process sequences of data in parallel and manage long-range dependencies, making them particularly effective for tasks like translation, text summarization, and more.\n",
       "\n",
       "**Key Features of Transformers:**\n",
       "1. **Self-Attention Mechanism**: This enables the model to weigh the relevance of different words within a sentence when producing an output, allowing context to be preserved even over long sequences.\n",
       "2. **Positional Encoding**: Since Transformers do not have a recurrent structure, they include positional encodings to give the model information about the order of the input data.\n",
       "3. **Multi-Head Attention**: This allows the model to focus on different parts of the sequence simultaneously, capturing a wider array of contextual relationships.\n",
       "\n",
       "Here's a simple summary of how Transformers work:\n",
       "\n",
       "1. Input text is tokenized and represented as embeddings.\n",
       "2. Positional encodings are added to the embeddings.\n",
       "3. The self-attention mechanism computes attention scores and applies them to obtain context-rich representations.\n",
       "4. Through stacked layers of multi-head attention and feedforward neural networks, the model processes the input.\n",
       "5. Finally, for tasks like translation or text generation, a decoder generates the output sequence.\n",
       "\n",
       "#### Examples of Transformers in Use:\n",
       "1. **Text Translation**: Google Translate utilizes Transformers to convert sentences from one language to another, effectively managing context across long sentences.\n",
       "2. **Text Summarization**: Summarization tools like BERTSUM leverage Transformers to condense long articles into concise summaries while retaining key information.\n",
       "3. **Chatbots**: Models like ChatGPT rely on Transformers to generate human-like responses based on the conversation context.\n",
       "\n",
       "---\n",
       "\n",
       "### In-Depth Explanation\n",
       "\n",
       "Transformers were introduced in the paper \"Attention is All You Need\" by Vaswani et al. in 2017. They revolutionized the field of NLP by overcoming limitations of previous models like recurrent neural networks (RNNs) and Long Short-Term Memory networks (LSTMs). Here’s a deeper look into their components:\n",
       "\n",
       "#### Self-Attention Mechanism\n",
       "- The self-attention mechanism calculates a score for each word in relation to all other words in the input sequence. \n",
       "- It provides insight into which words should be emphasized based on their relevance to the task at hand. Each word contributes to the final representation, weighted by its attention score.\n",
       "\n",
       "For instance, in the sentence “The cat sat on the mat,” the model could learn that \"cat\" and \"sat\" are closely related because the cat is the one performing the action.\n",
       "\n",
       "#### Positional Encoding\n",
       "- In RNNs, the structure inherently considers the sequence of data. To compensate for this in Transformers, positional encodings are added to word embeddings. These encodings are vectors that represent the position of each word in a sequence.\n",
       "  \n",
       "#### Multi-Head Attention\n",
       "- Multi-head attention functions by performing several self-attention operations in parallel, enabling the model to capture different contextual relationships. Each attention head may focus on different aspects of the input, resulting in richer contextual embeddings.\n",
       "\n",
       "#### Encoder-Decoder Architecture\n",
       "- Transformers consist of an encoder-decoder structure:\n",
       "  - **Encoder**: Takes input sequences and encodes the information into a context vector.\n",
       "  - **Decoder**: Takes the context vector and generates the output sequence, often conditioned on the previous words.\n",
       "\n",
       "Transformers have become foundational in various applications beyond NLP, influencing fields such as computer vision and even audio processing due to their adaptability. Their high efficiency in training, thanks to parallelization, has led to their widespread adoption in the development of numerous state-of-the-art models like BERT, GPT, and T5, thus cementing their place as a cornerstone in modern Deep Learning. \n",
       "\n",
       "Overall, the introduction of Transformers marked a paradigm shift, showcasing how attention mechanisms can streamline processing sequences, which continues to shape advancements in AI today."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Write your question here\n",
    "question = \"\"\"\n",
    "Can you explain me the concept of Transformers in Deep Learning, how they work and why Deep Learning relies on this in order to work ?\n",
    "\"\"\"\n",
    "\n",
    "# Choose your model. Available:\n",
    "# - GPT-4o : \"gpt-4o\"\n",
    "# - ollama : \"ollama\"\n",
    "# - claude-haiku-3.5 : \"claude-haiku\"\n",
    "# - gemini-flash-2.5 : \"gemini-2.5\"\n",
    "use_model = \"gpt-4o\"\n",
    "\n",
    "generate_answer(question, use_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ed02c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Understanding torch.masked and MaskedTensor in PyTorch\n",
       "\n",
       "## Quick Explanation\n",
       "\n",
       "A `MaskedTensor` in PyTorch is a specialized tensor that allows you to perform operations while selectively ignoring or masking certain elements based on a boolean mask. The key features are:\n",
       "\n",
       "1. It enables element-wise operations with partial data\n",
       "2. Allows selective computation on specific tensor elements\n",
       "3. Provides a way to handle missing or irrelevant data efficiently\n",
       "\n",
       "## Code Examples\n",
       "\n",
       "```python\n",
       "import torch\n",
       "\n",
       "# Basic Masked Tensor Example\n",
       "x = torch.tensor([1, 2, 3, 4, 5])\n",
       "mask = torch.tensor([True, False, True, False, True])\n",
       "\n",
       "# Creating a masked tensor\n",
       "masked_x = torch.masked.MaskedTensor(x, mask)\n",
       "\n",
       "# Performing operations\n",
       "result = masked_x.sum()  # Only sums masked elements\n",
       "print(result)  # Output: 9 (1 + 3 + 5)\n",
       "```\n",
       "\n",
       "```python\n",
       "# Advanced Masked Tensor Operation\n",
       "data = torch.tensor([[1, 2, 3], \n",
       "                     [4, 5, 6], \n",
       "                     [7, 8, 9]])\n",
       "mask = torch.tensor([[True, False, True], \n",
       "                     [False, True, False], \n",
       "                     [True, True, False]])\n",
       "\n",
       "masked_data = torch.masked.MaskedTensor(data, mask)\n",
       "mean = masked_data.mean()\n",
       "print(mean)  # Computes mean of only masked elements\n",
       "```\n",
       "\n",
       "## In-Depth Theoretical Explanation\n",
       "\n",
       "### Conceptual Understanding\n",
       "A `MaskedTensor` is essentially a data structure that:\n",
       "- Stores both the original tensor data\n",
       "- Maintains a corresponding boolean mask\n",
       "- Allows selective computation based on the mask\n",
       "\n",
       "### Key Mechanisms\n",
       "1. **Mask Creation**: \n",
       "   - Boolean tensor of same shape as original tensor\n",
       "   - `True` indicates elements to be included\n",
       "   - `False` indicates elements to be masked/ignored\n",
       "\n",
       "2. **Operation Principles**:\n",
       "   - Mathematical operations respect the mask\n",
       "   - Masked elements are effectively \"removed\" from computation\n",
       "   - Reduces computational overhead by skipping irrelevant elements\n",
       "\n",
       "### Use Cases\n",
       "- Handling missing data in machine learning\n",
       "- Selective tensor computations\n",
       "- Efficient data preprocessing\n",
       "- Implementing advanced neural network architectures\n",
       "\n",
       "### Technical Implementation\n",
       "```python\n",
       "# Internal Representation Concept\n",
       "class MaskedTensor:\n",
       "    def __init__(self, data, mask):\n",
       "        self.data = data    # Original tensor\n",
       "        self.mask = mask    # Boolean mask\n",
       "        self._validate_mask()\n",
       "    \n",
       "    def _validate_mask(self):\n",
       "        # Ensure mask matches tensor dimensions\n",
       "        assert self.data.shape == self.mask.shape\n",
       "```\n",
       "\n",
       "### Performance Considerations\n",
       "- Lower memory overhead\n",
       "- Faster computation by skipping masked elements\n",
       "- Native PyTorch implementation ensures GPU acceleration\n",
       "\n",
       "### Advanced Techniques\n",
       "1. Dynamic mask generation\n",
       "2. Mask propagation in complex neural networks\n",
       "3. Handling multi-dimensional masked tensors\n",
       "\n",
       "## Practical Recommendations\n",
       "- Always ensure mask and tensor have identical shapes\n",
       "- Use `torch.masked` for type-safe operations\n",
       "- Leverage masks for data cleaning and selective processing\n",
       "\n",
       "By understanding `MaskedTensor`, you gain a powerful tool for handling complex tensor operations with precision and efficiency."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"\"\"\n",
    "Can you explain me how torch.masked works and what is the concept of MaskedTensor ?\n",
    "\"\"\"\n",
    "\n",
    "# Choose your model. Available:\n",
    "# - GPT-4o : \"gpt-4o\"\n",
    "# - ollama : \"ollama\"\n",
    "# - claude-haiku-3.5 : \"claude-haiku\"\n",
    "# - gemini-flash-2.5 : \"gemini-2.5\"\n",
    "use_model = \"claude-haiku\"\n",
    "\n",
    "generate_answer(question, use_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "733aea92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hello there! Let's break down how to create a TensorFlow neural network using JavaScript and then how to deploy and run it inside a Virtual Machine on Google Cloud. It's a fantastic way to bring machine learning to web applications or server-side JavaScript environments.\n",
       "\n",
       "***\n",
       "\n",
       "### Quick Answer & Solution\n",
       "\n",
       "To create a TensorFlow neural network in JavaScript, you'll use the **TensorFlow.js** library. It allows you to build, train, and run ML models directly in the browser or on Node.js.\n",
       "\n",
       "To do this inside a VM in Google Cloud, you'll provision a **Google Compute Engine** virtual machine, install Node.js (if running server-side), and then execute your TensorFlow.js application there.\n",
       "\n",
       "Here's the general flow:\n",
       "\n",
       "1.  **Develop your TensorFlow.js model:** Write your neural network code using the `tfjs` library.\n",
       "2.  **Prepare your Google Cloud VM:** Create a Compute Engine instance, choose an operating system (like Ubuntu), and set up basic networking.\n",
       "3.  **Deploy and Run:** Install Node.js and the necessary TensorFlow.js packages on your VM, then run your JavaScript application. If it's a browser-based application, you'll serve the HTML/JS files from the VM.\n",
       "\n",
       "***\n",
       "\n",
       "### Multiple Examples\n",
       "\n",
       "Let's look at two practical examples: one for a server-side (Node.js) application and another for a browser-based application, both runnable on a Google Cloud VM.\n",
       "\n",
       "#### Example 1: Simple Linear Regression in Node.js (Server-Side on VM)\n",
       "\n",
       "This example demonstrates a basic linear regression model that predicts `y` from `x`. We'll run this directly as a Node.js script on your VM.\n",
       "\n",
       "**1. Create your JavaScript file (e.g., `linear_regression.js`):**\n",
       "\n",
       "```javascript\n",
       "// Import TensorFlow.js for Node.js\n",
       "const tf = require('@tensorflow/tfjs-node');\n",
       "\n",
       "async function runLinearRegression() {\n",
       "  // Define a simple model: y = mx + b\n",
       "  const model = tf.sequential();\n",
       "  model.add(tf.layers.dense({ units: 1, inputShape: [1] })); // One input, one output\n",
       "\n",
       "  // Compile the model with an optimizer and loss function\n",
       "  model.compile({ loss: 'meanSquaredError', optimizer: 'sgd' }); // SGD = Stochastic Gradient Descent\n",
       "\n",
       "  // Prepare some training data\n",
       "  // Our target function is roughly y = 2x + 1\n",
       "  const xs = tf.tensor2d([1, 2, 3, 4], [4, 1]); // Input features\n",
       "  const ys = tf.tensor2d([3, 5, 7, 9], [4, 1]); // Target labels\n",
       "\n",
       "  console.log('Starting model training...');\n",
       "  // Train the model\n",
       "  await model.fit(xs, ys, {\n",
       "    epochs: 500, // Number of times to iterate over the dataset\n",
       "    callbacks: {\n",
       "      onEpochEnd: (epoch, logs) => {\n",
       "        if (epoch % 100 === 0) {\n",
       "          console.log(`Epoch ${epoch}: Loss = ${logs.loss.toFixed(4)}`);\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "  });\n",
       "  console.log('Model training complete.');\n",
       "\n",
       "  // Make a prediction\n",
       "  const input = tf.tensor2d([5], [1, 1]);\n",
       "  const prediction = model.predict(input);\n",
       "  prediction.print(); // Output the prediction to the console\n",
       "\n",
       "  // You can also get the actual value\n",
       "  console.log(`Prediction for x = 5: ${prediction.dataSync()[0].toFixed(2)}`);\n",
       "}\n",
       "\n",
       "runLinearRegression();\n",
       "```\n",
       "\n",
       "**2. Steps to run on Google Cloud VM:**\n",
       "\n",
       "*   **Create a VM Instance:**\n",
       "    *   Go to Google Cloud Console > Compute Engine > VM instances.\n",
       "    *   Click \"CREATE INSTANCE\".\n",
       "    *   Choose a name (e.g., `tfjs-node-vm`).\n",
       "    *   Select a region and zone.\n",
       "    *   For \"Machine configuration\", a basic `e2-medium` or `e2-small` should be fine for this simple example.\n",
       "    *   For \"Boot disk\", select an OS like `Debian` or `Ubuntu`.\n",
       "    *   Click \"CREATE\".\n",
       "*   **SSH into the VM:**\n",
       "    *   Once the VM is running, click the \"SSH\" button next to your instance in the Console.\n",
       "*   **Install Node.js and npm:**\n",
       "    *   Update your package list:\n",
       "        ```bash\n",
       "        sudo apt update\n",
       "        ```\n",
       "    *   Install Node.js (using `nvm` is recommended for managing versions, but for simplicity, we'll use `apt` here):\n",
       "        ```bash\n",
       "        sudo apt install nodejs npm -y\n",
       "        ```\n",
       "    *   Verify installation:\n",
       "        ```bash\n",
       "        node -v\n",
       "        npm -v\n",
       "        ```\n",
       "*   **Create a project directory and install TensorFlow.js:**\n",
       "    ```bash\n",
       "    mkdir my-tfjs-app\n",
       "    cd my-tfjs-app\n",
       "    npm init -y # Initializes a package.json\n",
       "    npm install @tensorflow/tfjs-node\n",
       "    ```\n",
       "*   **Upload your `linear_regression.js` file:** You can use `scp` from your local machine or copy-paste the content directly into a new file on the VM using `nano` or `vi`.\n",
       "    ```bash\n",
       "    # On your local machine, from the directory containing linear_regression.js\n",
       "    gcloud compute scp linear_regression.js tfjs-node-vm:~/my-tfjs-app --zone=[YOUR_VM_ZONE]\n",
       "    ```\n",
       "    (Replace `[YOUR_VM_ZONE]` with the zone of your VM, e.g., `us-central1-a`).\n",
       "*   **Run the script on the VM:**\n",
       "    ```bash\n",
       "    node linear_regression.js\n",
       "    ```\n",
       "    You will see the training progress and the final prediction printed to the console.\n",
       "\n",
       "---\n",
       "\n",
       "#### Example 2: Simple Image Classification (Browser-Based on VM)\n",
       "\n",
       "This example shows a simple neural network for classifying two types of synthetic data (imagine two types of images). The model runs in the user's browser, but the HTML and JavaScript files are served from an HTTP server running on your Google Cloud VM.\n",
       "\n",
       "**1. Create your HTML file (e.g., `index.html`):**\n",
       "\n",
       "```html\n",
       "<!DOCTYPE html>\n",
       "<html>\n",
       "<head>\n",
       "    <title>TF.js Browser Classification</title>\n",
       "    <script src=\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest\"></script>\n",
       "    <style>\n",
       "        body { font-family: sans-serif; margin: 20px; }\n",
       "        #output { margin-top: 20px; font-weight: bold; }\n",
       "        button { padding: 10px 20px; font-size: 16px; cursor: pointer; }\n",
       "    </style>\n",
       "</head>\n",
       "<body>\n",
       "    <h1>TensorFlow.js Simple Classification</h1>\n",
       "    <p>This model classifies data points into two categories (0 or 1).</p>\n",
       "    <button id=\"trainButton\">Train Model</button>\n",
       "    <button id=\"predictButton\" disabled>Predict Random Data</button>\n",
       "    <div id=\"output\"></div>\n",
       "\n",
       "    <script>\n",
       "        let model;\n",
       "        let isTrained = false;\n",
       "        const outputDiv = document.getElementById('output');\n",
       "        const trainButton = document.getElementById('trainButton');\n",
       "        const predictButton = document.getElementById('predictButton');\n",
       "\n",
       "        async function createAndTrainModel() {\n",
       "            outputDiv.innerText = 'Creating and training model...';\n",
       "\n",
       "            // Define a simple sequential model\n",
       "            model = tf.sequential();\n",
       "            model.add(tf.layers.dense({ units: 10, activation: 'relu', inputShape: [2] })); // Two input features\n",
       "            model.add(tf.layers.dense({ units: 1, activation: 'sigmoid' })); // Binary classification (0 or 1)\n",
       "\n",
       "            // Compile the model\n",
       "            model.compile({\n",
       "                optimizer: tf.train.adam(0.01),\n",
       "                loss: 'binaryCrossentropy',\n",
       "                metrics: ['accuracy']\n",
       "            });\n",
       "\n",
       "            // Generate some synthetic data for classification\n",
       "            // Class 0: points around (0,0)\n",
       "            // Class 1: points around (1,1)\n",
       "            const numPoints = 100;\n",
       "            const xs_data = [];\n",
       "            const ys_data = [];\n",
       "\n",
       "            for (let i = 0; i < numPoints; i++) {\n",
       "                // Class 0\n",
       "                xs_data.push([Math.random() * 0.5, Math.random() * 0.5]);\n",
       "                ys_data.push(0);\n",
       "\n",
       "                // Class 1\n",
       "                xs_data.push([1 + Math.random() * 0.5, 1 + Math.random() * 0.5]);\n",
       "                ys_data.push(1);\n",
       "            }\n",
       "\n",
       "            const xs = tf.tensor2d(xs_data, [numPoints * 2, 2]);\n",
       "            const ys = tf.tensor2d(ys_data, [numPoints * 2, 1]);\n",
       "\n",
       "            // Train the model\n",
       "            await model.fit(xs, ys, {\n",
       "                epochs: 50,\n",
       "                shuffle: true,\n",
       "                callbacks: {\n",
       "                    onEpochEnd: (epoch, logs) => {\n",
       "                        outputDiv.innerText = `Epoch ${epoch}: Loss = ${logs.loss.toFixed(4)}, Accuracy = ${logs.acc.toFixed(4)}`;\n",
       "                    },\n",
       "                    onTrainEnd: () => {\n",
       "                        outputDiv.innerText += '\\nModel training complete!';\n",
       "                        isTrained = true;\n",
       "                        predictButton.disabled = false;\n",
       "                        trainButton.disabled = true;\n",
       "                    }\n",
       "                }\n",
       "            });\n",
       "        }\n",
       "\n",
       "        async function predictRandomData() {\n",
       "            if (!isTrained) {\n",
       "                outputDiv.innerText = 'Please train the model first!';\n",
       "                return;\n",
       "            }\n",
       "\n",
       "            // Generate a random test point\n",
       "            const testX = [Math.random() * 1.5, Math.random() * 1.5];\n",
       "            const input = tf.tensor2d([testX], [1, 2]);\n",
       "\n",
       "            // Make a prediction\n",
       "            const prediction = model.predict(input);\n",
       "            const classProb = prediction.dataSync()[0];\n",
       "            const predictedClass = classProb > 0.5 ? 1 : 0;\n",
       "\n",
       "            outputDiv.innerText = `\\nPrediction for [${testX[0].toFixed(2)}, ${testX[1].toFixed(2)}]: Class ${predictedClass} (Probability: ${classProb.toFixed(4)})`;\n",
       "        }\n",
       "\n",
       "        trainButton.addEventListener('click', createAndTrainModel);\n",
       "        predictButton.addEventListener('click', predictRandomData);\n",
       "    </script>\n",
       "</body>\n",
       "</html>\n",
       "```\n",
       "\n",
       "**2. Steps to run on Google Cloud VM:**\n",
       "\n",
       "*   **Follow VM creation and SSH steps from Example 1.**\n",
       "*   **Install a simple HTTP server on the VM:**\n",
       "    ```bash\n",
       "    sudo npm install -g http-server\n",
       "    ```\n",
       "*   **Create a directory and upload `index.html`:**\n",
       "    ```bash\n",
       "    mkdir my-web-app\n",
       "    cd my-web-app\n",
       "    # Upload index.html here using scp or copy-paste\n",
       "    gcloud compute scp index.html tfjs-node-vm:~/my-web-app --zone=[YOUR_VM_ZONE]\n",
       "    ```\n",
       "*   **Open Firewall Port (important!):**\n",
       "    *   Go to Google Cloud Console > VPC network > Firewall.\n",
       "    *   Click \"CREATE FIREWALL RULE\".\n",
       "    *   Name it (e.g., `allow-http-8080`).\n",
       "    *   Set \"Targets\" to \"All instances in the network\" or \"Specified target tags\" if you've tagged your VM.\n",
       "    *   Set \"Source IPv4 ranges\" to `0.0.0.0/0` (for public access, be cautious in production).\n",
       "    *   For \"Protocols and ports\", select \"Specified protocols and ports\" and enter `tcp:8080`.\n",
       "    *   Click \"CREATE\".\n",
       "*   **Start the HTTP server on the VM:**\n",
       "    ```bash\n",
       "    cd ~/my-web-app\n",
       "    http-server -p 8080\n",
       "    ```\n",
       "    This will serve your `index.html` file on port 8080.\n",
       "*   **Access in your browser:**\n",
       "    *   Find your VM's \"External IP\" in the Compute Engine instances list.\n",
       "    *   Open your web browser and navigate to `http://[YOUR_VM_EXTERNAL_IP]:8080`.\n",
       "    *   You should see the web page. Click \"Train Model\" to start the training in your browser, then \"Predict Random Data\" to test it.\n",
       "\n",
       "***\n",
       "\n",
       "### In-Depth Theory: TensorFlow.js and Google Cloud Integration\n",
       "\n",
       "Let's dive deeper into what's happening and why these tools are so powerful together.\n",
       "\n",
       "#### TensorFlow.js: Machine Learning in JavaScript\n",
       "\n",
       "TensorFlow.js is an open-source JavaScript library developed by Google for machine learning. It's a complete rewrite of TensorFlow for the JavaScript ecosystem, meaning you can use the same core concepts and APIs you might be familiar with from Python's TensorFlow, but all within JavaScript.\n",
       "\n",
       "**Key Features and Concepts:**\n",
       "\n",
       "1.  **Browser-based ML:** This is arguably its most exciting feature. TensorFlow.js can leverage a user's device (CPU, and more importantly, GPU via WebGL or WebGPU) to run ML models directly in the browser. This enables:\n",
       "    *   **Interactive ML experiences:** Real-time predictions based on user input (e.g., webcam feed).\n",
       "    *   **Privacy:** Data never leaves the user's device for inference.\n",
       "    *   **Offline capabilities:** Models can run without an internet connection once loaded.\n",
       "    *   **Reduced server load:** Offload computation to client devices.\n",
       "2.  **Node.js for Server-side ML:** TensorFlow.js also has a backend optimized for Node.js (`@tensorflow/tfjs-node`). This package includes C++ bindings to the TensorFlow library, allowing it to take advantage of native hardware acceleration (like CPUs and NVIDIA GPUs via CUDA) on your server. This is perfect for:\n",
       "    *   **Backend inference:** Running models on your server for API endpoints.\n",
       "    *   **Model training:** Training larger models or performing more intensive computations than what's feasible in a browser.\n",
       "    *   **Data processing:** Preprocessing data before feeding it into models.\n",
       "3.  **Tensors:** The fundamental data structure in TensorFlow.js (and TensorFlow generally) is the `tf.Tensor`. Tensors are multi-dimensional arrays, similar to NumPy arrays. All operations in TensorFlow.js are performed on tensors.\n",
       "4.  **Operations (Ops):** These are mathematical computations performed on tensors (e.g., addition, multiplication, matrix operations). TensorFlow.js provides a vast library of these operations.\n",
       "5.  **Models (Sequential & Functional API):**\n",
       "    *   **`tf.sequential()`:** The simplest way to build a neural network, where layers are stacked one after another. Ideal for feed-forward networks.\n",
       "    *   **`tf.model()` (Functional API):** Offers more flexibility for complex architectures, allowing for multiple inputs/outputs, shared layers, and non-linear topologies (like residual connections).\n",
       "6.  **Optimizers:** Algorithms that adjust the model's internal parameters (weights and biases) during training to minimize the `loss` function. Common ones include `sgd` (Stochastic Gradient Descent), `adam`, `rmsprop`.\n",
       "7.  **Loss Functions:** A measure of how well the model is performing. During training, the goal is to minimize this value. Examples include `meanSquaredError` (for regression) and `binaryCrossentropy` or `categoricalCrossentropy` (for classification).\n",
       "8.  **Pre-trained Models:** TensorFlow.js provides access to a variety of pre-trained models (e.g., MobileNet for image classification, Universal Sentence Encoder for text embeddings) that you can use directly or fine-tune for your specific tasks (transfer learning).\n",
       "9.  **Model Conversion:** You can convert models trained in Python's TensorFlow (Keras) into a TensorFlow.js format, allowing you to leverage powerful training environments and then deploy them in JavaScript.\n",
       "\n",
       "#### Google Cloud Compute Engine: Your Virtual Machine Powerhouse\n",
       "\n",
       "Google Compute Engine is the Infrastructure as a Service (IaaS) component of Google Cloud that allows you to run virtual machines (VMs) on Google's infrastructure. Think of it as renting a computer in Google's data center that you can fully control.\n",
       "\n",
       "**Why use Compute Engine for ML/TensorFlow.js?**\n",
       "\n",
       "1.  **Scalability:** Easily create, resize, and delete VMs as your needs change. You can start with a small machine for development and scale up to powerful machines with many CPUs or GPUs for training.\n",
       "2.  **Reliability:** Google's global infrastructure is designed for high availability and redundancy, ensuring your applications are always accessible.\n",
       "3.  **Global Reach:** Deploy your VMs in various regions and zones around the world, placing your applications closer to your users for lower latency.\n",
       "4.  **Integration with GCP Services:** Compute Engine integrates seamlessly with other Google Cloud services like Cloud Storage (for data), Cloud Monitoring (for performance), Cloud Load Balancing (for distributing traffic), and more.\n",
       "5.  **Customization:** You have full control over the operating system, software stack, and machine configuration (CPU, memory, GPU, disk size). This is crucial for ML workloads where specific libraries or GPU drivers might be needed.\n",
       "\n",
       "**Key Concepts for ML on Compute Engine:**\n",
       "\n",
       "1.  **Instances:** These are your individual virtual machines.\n",
       "2.  **Machine Types:** Predefined or custom configurations of CPU, memory, and optional GPUs. For TensorFlow.js (Node.js), if you're doing heavy training or inference, consider machine types with more vCPUs and RAM. For GPU acceleration with `@tensorflow/tfjs-node-gpu`, you'll need to attach a GPU (e.g., NVIDIA T4, V100) to your instance.\n",
       "3.  **Boot Disk:** The primary storage device for your VM's operating system and installed software. You can choose different disk types (standard persistent disk, SSD persistent disk) based on performance needs.\n",
       "4.  **Networking:** Each VM gets an internal IP and often an external IP. Firewall rules are essential to control which traffic can reach your VM (e.g., allowing HTTP/HTTPS, SSH).\n",
       "5.  **SSH:** Secure Shell is the primary way to access and manage your Linux-based VMs remotely.\n",
       "\n",
       "#### How They Work Together\n",
       "\n",
       "*   **Client-Side ML (Browser-based TF.js):** Your Google Cloud VM acts as a web server (e.g., using Node.js with Express, or `nginx`, `apache`, `http-server`). It serves your HTML, CSS, and JavaScript files to the user's browser. The TensorFlow.js model then runs *on the user's device*, utilizing their local CPU/GPU for inference and potentially training. The VM's role here is primarily hosting the web application.\n",
       "*   **Server-Side ML (Node.js TF.js):** Your Google Cloud VM runs a Node.js application that uses `@tensorflow/tfjs-node` or `@tensorflow/tfjs-node-gpu`. This application can:\n",
       "    *   **Train models:** Leverage the VM's CPUs or attached GPUs to train models more quickly than a browser could.\n",
       "    *   **Perform inference:** Provide an API endpoint where other applications can send data, and the VM performs predictions using the TensorFlow.js model. This is useful when data is sensitive, too large for the client, or when you need consistent performance regardless of the client's device.\n",
       "    *   **Batch processing:** Process large datasets in batches using ML models.\n",
       "\n",
       "By understanding both TensorFlow.js and Google Compute Engine, you gain the flexibility to deploy powerful machine learning capabilities in a scalable and robust cloud environment, whether you're targeting client-side experiences or robust server-side services."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"\"\"\n",
    "How to create a Tensorflow neural network using Javascript and how to do it in a VM inside Google Cloud ?\n",
    "\"\"\"\n",
    "\n",
    "# Choose your model. Available:\n",
    "# - GPT-4o : \"gpt-4o\"\n",
    "# - ollama : \"ollama\"\n",
    "# - claude-haiku-3.5 : \"claude-haiku\"\n",
    "# - gemini-flash-2.5 : \"gemini-flash-2.5\"\n",
    "use_model = \"gemini-flash-2.5\"\n",
    "\n",
    "generate_answer(question, use_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
